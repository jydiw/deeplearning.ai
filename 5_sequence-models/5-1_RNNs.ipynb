{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why Sequence Models\r\n",
    "\r\n",
    "Some examples of sequence data:\r\n",
    "- speech recognition\r\n",
    "- music generation\r\n",
    "- sentiment classification\r\n",
    "- DNA sequence analysis\r\n",
    "- machine translation\r\n",
    "- video activity recognition\r\n",
    "- name entity recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notation\r\n",
    "\r\n",
    "Example:\r\n",
    "- x: Harry Potter and Hermione Granger invented a new spell.\r\n",
    "  - x^<1>, x^<2>, ... x^\\<t\\>\r\n",
    "- y: 1 1 0 1 1 0 0 0 0\r\n",
    "- recall that x^(i) is the ith example, so x^(i)\\<t\\> is the tth word in the ith example\r\n",
    "\r\n",
    "How would we represent individual words in a sentence?\r\n",
    "\r\n",
    "- establish a \"vocabulary\" (or dictionary)\r\n",
    "- each x^/<t/> is a one-hot vector from this dictionary\r\n",
    "- unknown words can be given a temporary token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks\r\n",
    "\r\n",
    "- Standard neural networks can't accommodate varying lengths in the input and output (eg: sentences and their translations aren't the same length all the time).\r\n",
    "- no feature sharing--ideally we want a recognized word to be learned across different portions of text\r\n",
    "\r\n",
    "(Unidirectional) Recurring neural networks read each word vector serially by also passing through the activation value of the previous layer.\r\n",
    "\r\n",
    "- shared parameters between each layer\r\n",
    "\r\n",
    "### Forward Propagation\r\n",
    "\r\n",
    "a^<1> = g(Waa a<0> + Wax x<1> + ba)  \r\n",
    "yhat<1> = g(Wya a<1> + by)\r\n",
    "\r\n",
    "where Wya means an a-like quantity is used to compute a y-like quantity\r\n",
    "\r\n",
    "we can simplify the notation a bit:\r\n",
    "\r\n",
    "a/<t/> = g(Wa[a<t-1>, x/<t/>]) + ba)\r\n",
    "yhat<t> = g(Wy a<t> + by)\r\n",
    "\r\n",
    "### Back Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Types of RNNs\r\n",
    "\r\n",
    "Tx is usually not the same length as Ty.\r\n",
    "\r\n",
    "- sentiment classification\r\n",
    "- machine translation\r\n",
    "\r\n",
    "Types:\r\n",
    "- one-to-one (a standard NN))\r\n",
    "- Many-to-one\r\n",
    "- one-to-many\r\n",
    "- many-to-many (but different)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Model and Sequence Generation\r\n",
    "\r\n",
    "Speech Recognition:\r\n",
    "- the apple and pair salad vs. the apple and pear salad\r\n",
    "- language model assigns probability to the sentence being formed\r\n",
    "\r\n",
    "steps:\r\n",
    "- training set: large corpus of text\r\n",
    "- example: tokenized sentence (with end-of-sentence token)\r\n",
    "- models chance of a particular sequence of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling Novel Sequences\r\n",
    "\r\n",
    "From a starting word, we randomly sample from the softmax output of the first word\r\n",
    "\r\n",
    "can also have character-level language model\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanishing gradients with RNNs\r\n",
    "\r\n",
    "if you have a very deep neural network, the gradient has a tough time backpropagating all the way to the first layer.\r\n",
    "\r\n",
    "local influences: yhat3 is mainly influenced by values close to yhat3.\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gated Recurrent Unit (GRU)\r\n",
    "\r\n",
    "ct = at\r\n",
    "\r\n",
    "we come up with a candidate, and then the gate will determine if we keep it.\r\n",
    "\r\n",
    "ct = Gammau * c~t + (1-Gammau) * c/<t/-1/>\r\n",
    "\r\n",
    "WHen gate = 0, that means we keep the candidate value.\r\n",
    "\r\n",
    "** READ MORE ON THIS, THIS VIDEO WAS CONFUSING **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long Short-term Memory (LSTM)\r\n",
    "\r\n",
    "GRU is gaining in momentum due to it being simpler to compute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional RNN\r\n",
    "\r\n",
    "Has both forward and backward activation (af and ab) in forward propagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep RNNs\r\n",
    "\r\n",
    "Usually not very many \"horizontally\" connected RNN layers.\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "nbconvert_exporter": "python",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}