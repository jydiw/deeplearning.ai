{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Representation\r\n",
    "\r\n",
    "- one-hot representation\r\n",
    "  - cons: treats each word as its own separate entity\r\n",
    "    - inner product of any two one-hot vectors is zero, so there are no ways for a model to learn relationships\r\n",
    "  - not easy to generalize word meanings\r\n",
    "- featurized representation\r\n",
    "  - have features like gender, age, etc.\r\n",
    "  - these features could be hand-selected, or they could be learned via neural network\r\n",
    "\r\n",
    "visualizing word embeddings\r\n",
    "- figuring out how to reduce the dimensionality of the features to 2D\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Word Embeddings\r\n",
    "\r\n",
    "- learn word embeddings from large text corpuses (1-100B words)\r\n",
    "- transfer embedding to new tasks with smaller training sets (100k words, eg)\r\n",
    "- continue to fine-tune the word embeddings with new data\r\n",
    "\r\n",
    "\"face encoding\" and \"word embedding\" have similar meanings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Properties of Word Embeddings\r\n",
    "\r\n",
    "- featurized word embeddings can use vector similarity to determine analogous relationships\r\n",
    "  - cosine similarity -- u_transpose * v / length_u * length_v\r\n",
    "  - euclidian distance -- length_u-v_squared\r\n",
    "- eg: eman - ewoman ~ eking - equeen\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Matrix\r\n",
    "\r\n",
    "say we have a 10k-word vocabulary (a, aaron, ..., orange, ..., zulu, `unknown`)\r\n",
    "\r\n",
    "we have a 300 (n-features) by 10001 dimensional matrix E.\r\n",
    "\r\n",
    "if we have E(300, 10k) * Ohv(10k, 1) = (300, 1) e6257 corresponding to the word orange.\r\n",
    "\r\n",
    "E * oj = ej (the embedding for word j)\r\n",
    "\r\n",
    "What we do is initialize E randomly and then use gradient descent to learn word embeddings.\r\n",
    "\r\n",
    "It's not actually that efficient to just do matrix multiplication. we typically just look up the column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Word Embeddings: Word2Vec and GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Word Embeddings\r\n",
    "\r\n",
    "I want a glass of orange `blank`.  \r\n",
    "```[4343, 9665, 1, 3852, 6163, 6257, blank]```\r\n",
    "\r\n",
    "I: e4343 = E * o4343  \r\n",
    "want: e9665 = E * o9665  \r\n",
    "etc.\r\n",
    "\r\n",
    "We can then feed all the e vectors into a hidden layer which then feeds into a softmax, which can then select the most probable word to follow.\r\n",
    "\r\n",
    "We can also train a context (eg last 4 words, 4 words on left and right, \"nearby 1 word\" skipgram)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec\r\n",
    "\r\n",
    "We choose context and target words:\r\n",
    "\r\n",
    "I want a glass of orange juice to go along with my cereal.\r\n",
    "\r\n",
    "|context|target|\r\n",
    "|---|---|\r\n",
    "|orange|juice|\r\n",
    "|orange|along|\r\n",
    "|orange|my|\r\n",
    "\r\n",
    "context c (orange, 6257) -> target t (juice, 4834)\r\n",
    "\r\n",
    "find e6257 and e4834\r\n",
    "\r\n",
    "feed e vectors to softmax node to output yhat\r\n",
    "\r\n",
    "softmax: p(t|c) = e^(thetattranspose ec) / sum of all vocab (e^(thetajtranspose ec))\r\n",
    "\r\n",
    "main drawback: very expensive to calculate softmax\r\n",
    "\r\n",
    "can use hierarchical softmax to lessen computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative Sampling\r\n",
    "\r\n",
    "Defining a new learning problem\r\n",
    "\r\n",
    "|context|word|target|\r\n",
    "|---|---|---|\r\n",
    "|orange|juice|1|\r\n",
    "|orange|king|0|\r\n",
    "|orange|book|0|\r\n",
    "|orange|the|0|\r\n",
    "\r\n",
    "We create supervised learning problem where we insert pair of words, predict whether they are associated with each other in context.\r\n",
    "\r\n",
    "e6257 fed into 10k neuron layer that predicts association, but we only train a selected number of negative examples.\r\n",
    "\r\n",
    "how can we choose our negative samples?\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe\r\n",
    "\r\n",
    "global vectors for word representation\r\n",
    "\r\n",
    "see how related words are to each other by seeing how often they appear next to each other\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}