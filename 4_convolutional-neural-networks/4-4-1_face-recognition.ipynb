{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Recognition\n",
    "\n",
    "**Summary:**\n",
    "\n",
    "- one-shot learning\n",
    "- siamese network\n",
    "- triplet loss\n",
    "- face verification and binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Face recognition + liveness detection = real-time face recognition\n",
    "\n",
    "Face verification vs. face recognition\n",
    "\n",
    "verification:\n",
    "- input image + label\n",
    "- output: whether input image is that of the claimed person\n",
    "\n",
    "recognition:\n",
    "- has a database of K persons\n",
    "- get an input image\n",
    "- output ID if the image is any of the K persons\n",
    "\n",
    "## One Shot Learning\n",
    "\n",
    "most face-recognition applications need to be able to recognize a person given one single training example.\n",
    "\n",
    "image -> CNN -> softmax (C + 1)  \n",
    "doesn't work well\n",
    "\n",
    "one shot learning: only get one chance for model to make correct prediction\n",
    "\n",
    "we don't want to retrain the model every time\n",
    "\n",
    "learn similarity function to solve the problem of one shot learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Siamese Network\n",
    "\n",
    "Instead of feeding encoding into softmax output layer, we can design NN to define a vector encoding $f(x^{(1)})$ and learn parameters such that if $x^{(i)}$ and $x^{(j)}$ are the same person, the distance $d$ between the encoding vectors is small:\n",
    "\n",
    "$$d\\!\\left(x^{(i)}, x^{(j)}\\right) = \\big\\|f\\big(x^{(1)}\\big) - f\\big(x^{(j)}\\big)\\big\\|^{2}_{2}$$\n",
    "\n",
    "It follows that if $x^{(i)}$ and $x^{(j)}$ are not the same person, you'd want $d$ to be large. You can then use back-propagation in order to make sure these conditions are met."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triplet Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to learn the parameters of this NN is to define an applied gradient descent on the *triplet loss* function. Let's define three different images:\n",
    "\n",
    "- **A**: the anchor image -- a ground-truth image on which we make comparisons\n",
    "- **P**: a positive example\n",
    "- **N**: a negative example\n",
    "\n",
    "From here, we want the the distance between **A** and **P** to be small, and **A** and **N** to be large:\n",
    "\n",
    "$$\n",
    "d\\!\\left(\\mathbf{A}, \\mathbf{P}\\right) \\leq d\\!\\left(\\mathbf{A}, \\mathbf{N}\\right)\n",
    "$$\n",
    "\n",
    "However, consider a case where a NN encodes everything to some number $n$; that would still be able to satisfy the condition:\n",
    "\n",
    "$$\n",
    "d\\!\\left(\\mathbf{A}, \\mathbf{P}\\right) - d\\!\\left(\\mathbf{A}, \\mathbf{N}\\right) \\leq 0\n",
    "$$\n",
    "\n",
    "To prevent this, we can add a *margin* hyperparameter $\\alpha$ such that:\n",
    "\n",
    "$$\n",
    "d\\!\\left(\\mathbf{A}, \\mathbf{P}\\right) - d\\!\\left(\\mathbf{A}, \\mathbf{N}\\right) + \\alpha \\leq 0\n",
    "$$\n",
    "\n",
    "More formally, we want to define a loss function such that we minimize:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\mathbf{A}, \\mathbf{P}, \\mathbf{N}) = \\mathrm{max}\\big(d\\!\\left(\\mathbf{A}, \\mathbf{P}\\right) - d\\!\\left(\\mathbf{A}, \\mathbf{N}\\right) + \\alpha, 0\\big)$$$$\n",
    "\\mathcal{J} = \\sum_{i=1}^{m} \\mathcal{L}\\big(\\mathbf{A^{(i)}}, \\mathbf{P^{(i)}}, \\mathbf{N^{(i)}}\\big)\n",
    "$$\n",
    "\n",
    "For the purpose of training, you need multiple images of the same person.\n",
    "\n",
    "During training, if **A**, **P**, and **N** are chosen randomly, then the training condition is easily satisfied. You want to choose such that $d\\!\\left(\\mathbf{A}, \\mathbf{P}\\right) \\approx d\\!\\left(\\mathbf{A}, \\mathbf{N}\\right)$\n",
    "\n",
    "Modern face recognition devices are trained on very large datasets, on the order of millions or even hundreds of millions of images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face Verification and Binary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative to the triplet loss training function would be for the NN to learn the similarity function:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\sigma \\big( \\sum_{k=1}^{n} w_k \\, \\big | f \\big(x^{(1)}\\big)_k - f\\big(x^{(j)}\\big)_k\\big | + b \\big)\n",
    "$$\n",
    "\n",
    "There are other variations, such as the $\\chi$ squared formula\n",
    "\n",
    "When a new facial input comes in, compare new encoding to pre-computed encoding, then run logistic function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
