{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4-3-1-object-localization.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP+l6ywIrq0YOdiBxble0Bv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jydiw/deeplearning.ai/blob/master/drafts/4_3_1_object_localization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKSZ1ED66ZKg",
        "colab_type": "text"
      },
      "source": [
        "# Object Detection\n",
        "\n",
        "In order to talk about object detection, we must start with object localization:\n",
        "\n",
        "- classification\n",
        "- classification with localization\n",
        "- detection\n",
        "\n",
        "# classification with localization\n",
        "\n",
        "sample softmax output:\n",
        "- 1 - pedestrian\n",
        "- 2 - car\n",
        "- 3 - motorcycle\n",
        "- 4 - background\n",
        "\n",
        "you can also have your network output four more numbers to define a \"bounding box\" of the detected object.\n",
        "\n",
        "> upper left: (0,0)\n",
        "> lower right (1,1)\n",
        "\n",
        "$(b_x, b_y)$ is midpoint of bounding box\n",
        "$(b_w, b_h)$ is width and height of bounding box\n",
        "\n",
        "sample output vector:\n",
        "\n",
        "$$\n",
        "\\mathbf{y} = \\begin{bmatrix}\n",
        "p_c \\\\\n",
        "b_x \\\\\n",
        "b_y \\\\\n",
        "b_h \\\\\n",
        "b_w \\\\\n",
        "c_1 \\\\\n",
        "c_2 \\\\\n",
        "c_3\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "where:\n",
        "\n",
        "$p_c$ probability of object (not background) present in image\n",
        "$c_i$ whether it belongs in any of the classes\n",
        "\n",
        "Loss can then be calculated as:\n",
        "\n",
        "$$loss(\\hat{y}, y) =\n",
        "\\begin{cases}\n",
        "\\displaystyle \\sum_{i=1}^{8}(\\hat{y}_i- y_i)^2 & \\quad \\text{if} \\ \\ y_1 = 1\\\\\n",
        "(\\hat{y}_1- y_1)^2 & \\quad \\text{if}\\ \\ y_1 = 0\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "In practice, log feature loss for c_i, squarred error for bounding box, and log-loss for pc.\n",
        "\n",
        "# landmark detection\n",
        "\n",
        "You can modify the output label to detect other landmarks, such as faces\n",
        "\n",
        "# object detection\n",
        "\n",
        "sliding windows detection: start with small \"window\" of main image and feed that crop into convnet\n",
        "\n",
        "repeat process with slightly larger window\n",
        "\n",
        "huge disadvantage: computational cost.\n",
        "so many different crops for a single image.\n",
        "\n",
        "coarse stride size could reduce computation but reduce performance as you could skip over objects in the image\n",
        "\n",
        "## convolutional implementation of sliding windows\n",
        "\n",
        "instead of a fully connected layer, convolve using a filter with as many filters as neurons in your fc layer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UK3k5nAd5LRg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ULpZNC_6W4e",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}